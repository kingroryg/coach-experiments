global:
  base_url: "http://127.0.0.1:8080"
  prompt_file: "prompts/endpoint_security_eval.jsonl"
  output_root: "results"
  timeout_s: 300
  warmup_prompts: 2
  samples_per_prompt: 1
  llama_server_bin: "${LLAMA_SERVER_BIN:-llama-server}"
  model_path: "LiquidAI/LFM2.5-1.2B-Instruct-GGUF"

runs:
  - name: "q4_threads2_ctx4096"
    model_path: "models/LiquidAI__LFM2.5-1.2B-Instruct-GGUF/LFM2.5-1.2B-Instruct-Q4_K_M.gguf"
    env:
      LLAMA_SERVER_MODE: "python"
      THREADS: "2"
      CTX_SIZE: "4096"
      N_GPU_LAYERS: "0"
      TEMPERATURE: "0"
      TOP_P: "1"
      GGML_METAL_DISABLE: "1"
      EXTRA_ARGS: ""
